# Leveraging Machine Learning in Distributed Systems  
## Distributed k-Nearest Neighbors (kNN) using Ray Framework

## üìå Overview

Traditional machine learning algorithms often face scalability challenges when applied to large datasets. The **k-Nearest Neighbors (kNN)** algorithm, while simple and effective, becomes computationally expensive due to its distance-based calculations and high memory usage.

This project implements a **distributed version of the kNN algorithm using the Ray framework**, enabling parallel execution across multiple workers and significantly improving performance and scalability.

---

## üéØ Objectives

- Analyze scalability limitations of traditional kNN  
- Implement parallel processing using Ray  
- Reduce execution time through distributed computation  
- Study task scheduling and resource utilization in distributed systems  

---

## üß† About k-Nearest Neighbors (kNN)

kNN is a supervised learning algorithm used for classification and regression tasks.

### Working Principle
1. Compute distance between query point and all training samples  
2. Select the *k* nearest neighbors  
3. Predict output using majority voting or averaging  

### Advantages
- Simple and intuitive  
- No explicit training phase  
- Supports multiple distance metrics  
- Effective for non-linear decision boundaries  

### Challenges
- High computational cost for large datasets  
- Memory-intensive (lazy learning)  
- Poor scalability  
- Curse of dimensionality  

These challenges make kNN an ideal candidate for distributed optimization.

---

## ‚öôÔ∏è Why Ray?

**Ray** is an open-source distributed computing framework designed for scaling Python applications.

### Key Features
- Python-native parallelism  
- Simple task and actor abstractions  
- Efficient scheduling and resource management  
- Seamless scaling from single machine to cluster  

Ray enables developers to build distributed systems without complex infrastructure management.

---

## üß∞ Technology Stack

- **Language:** Python  
- **Framework:** Ray  
- **Libraries:** NumPy, Pandas, SciPy, scikit-learn  
- **Python Version:** 3.10.9  

### System Configuration
- **CPU:** Intel i5 (4 cores, 8 threads)  
- **RAM:** 8 GB  
- **GPU:** NVIDIA GeForce MX250 (4 GB)  

---

## üß™ Implementation

### Sequential kNN (Without Ray)
- Distance computation executed sequentially  
- High execution time for large datasets  
- Limited CPU utilization

- In the sequential approach, all computations are performed on a single process. Distance calculations and predictions occur sequentially, leading to higher execution time for large datasets.

```python
# Train and time KNN (without Ray)
def train_and_time_knn(X_train, y_train, X_test, y_test, n_neighbors=3):
    knn = KNeighborsClassifier(n_neighbors=n_neighbors)

    start_time = time.time()
    knn.fit(X_train, y_train)        # Train KNN model
    y_pred = knn.predict(X_test)     # Predict on test data
    end_time = time.time()

    accuracy = accuracy_score(y_test, y_pred)
    time_taken = end_time - start_time

    return accuracy, time_taken
```

### Distributed kNN (With Ray)
- Distance calculations parallelized using Ray tasks  
- Workload distributed across multiple workers  
- Improved CPU utilization and throughput  

---
```python
# Parallel KNN using Ray
def train_and_time_knn_parallel(
    X_train, y_train, X_test, y_test,
    n_neighbors=3, num_actors=5
):
    # Split training data across multiple actors
    X_train_splits = np.array_split(X_train, num_actors)
    y_train_splits = np.array_split(y_train, num_actors)

    start_time = time.time()

    # Launch parallel computation
    futures = [
        knn_worker.remote(
            X_train_splits[i],
            y_train_splits[i],
            X_test,
            n_neighbors
        )
        for i in range(num_actors)
    ]

    # Collect predictions from all workers
    all_predictions = ray.get(futures)

    # Combine predictions using majority voting
    y_pred_combined = mode(
        all_predictions, axis=0
    )[0].flatten().astype(int)

    end_time = time.time()

    accuracy = accuracy_score(y_test, y_pred_combined)
    time_taken = end_time - start_time

    return accuracy, time_taken

```
## üìä Experimental Results

Performance comparison was conducted using:

- Sequential kNN vs Distributed kNN  
- Values of k = 3 and k = 5  

### Results
- Parallel execution across **10+ workers**  
- Approximately **40% reduction in computation time**  
- Improved system throughput and scalability  

---

## üöÄ Key Highlights

- Implemented distributed kNN using Ray framework  
- Executed parallel workloads across 10+ workers  
- Reduced execution time by ~40% compared to sequential approach  
- Analyzed task scheduling and resource utilization  
- Improved scalability for large-scale datasets  

---

## üîÆ Future Work

- Deploy Ray clusters in cloud environments  
- Enable GPU-accelerated distributed kNN  
- Compare performance with Apache Spark  
- Apply system to real-world use cases such as:
  - Recommendation systems  
  - Image recognition  
  - Anomaly detection  

---

## ‚úÖ Conclusion

This project demonstrates that **distributed kNN using Ray significantly improves execution efficiency and scalability** while preserving model accuracy. Ray‚Äôs distributed architecture provides an effective solution for overcoming the limitations of traditional kNN in large-scale machine learning scenarios.

---

## üìö References

- Ray Documentation: https://docs.ray.io  
- Learning Ray ‚Äî O‚ÄôReilly Media  
- GeeksforGeeks: k-Nearest Neighbors Algorithm  

